Agentic Sales Forecasting System using MCP and Local LLMs
(MCP + Local LLM + Streamlit + Scikit-Learn)

This project implements a fully agentic AI system that uses Anthropicâ€™s **Model Context Protocol (MCP)** to allow a local LLM (running via **Ollama**) to autonomously call Python tools for:

âœ”ï¸ Dataset discovery  
âœ”ï¸ Model training  
âœ”ï¸ Forecast generation  
âœ”ï¸ Result explanation  

The system enables end-to-end sales forecasting through natural language like:

> â€œTrain a model on sample_sales.csv and forecast the next 6 months.â€

---

ğŸš€ Features

ğŸ”§ 1. MCP-Powered Python Tooling
A dedicated MCP server exposes modular ML tools:
- `list_datasets()`
- `train_sales_model(dataset_name, date_column, target_column)`
- `forecast_sales(model_id, horizon_months)`

Each tool is typed, schema-validated, and returns structured JSON.

---

ğŸ¤– 2. Local LLM Agent (via Ollama)
- Uses `gpt-oss:120b-cloud` (or any Ollama model)
- LLM receives dynamically available tools
- Decides when to call a tool using strict JSON schema
- Executes multi-step workflows autonomously

---

ğŸ’¬ 3. Streamlit Conversational Interface
A conversational UI that:
- Stores chat history  
- Sends user queries to the LLM agent  
- Shows step-by-step tool execution  
- Produces ML forecasts + explanations  

---

ğŸ“ˆ 4. ML Forecasting Pipeline
- Loads time-series sales dataset  
- Generates time index  
- Trains regression model  
- Produces multi-step forecasts  
- Provides AI-generated trend explanation  

---

ğŸ— Project Structure

```
mcp_ollama_sales_forecast/
â”‚
â”œâ”€â”€ streamlit_ollama_mcp_app.py       Frontend conversational UI
â”œâ”€â”€ mcp_sales_server.py               MCP server exposing ML tools
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ sample_sales.csv
â”‚
â”œâ”€â”€ models/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

ğŸ›  Installation

1ï¸âƒ£ Create virtual environment
```
python -m venv .venv
source .venv/bin/activate
```

2ï¸âƒ£ Install dependencies
```
pip install -r requirements.txt
```

3ï¸âƒ£ Pull Ollama model
```
ollama pull gpt-oss:120b-cloud
```

4ï¸âƒ£ Run the app
```
python -m streamlit run streamlit_ollama_mcp_app.py
```

---

â–¶ï¸ How It Works

1. User submits a natural-language query  
2. Streamlit sends it to the agent  
3. LLM inspects available MCP tools  
4. LLM emits JSON tool call  
5. Agent executes tool via MCP  
6. Tool returns structured JSON  
7. LLM explains the result  

Complete autonomous reasoning and execution loop.

---

ğŸ§ª Example Prompts

```
What datasets are available?
Train a model on sample_sales.csv and forecast 6 months.
Explain whether sales are rising.
```

---

ğŸ¯ Skills Demonstrated

- Agentic AI system design  
- MCP integration  
- Local LLM tool-calling  
- Streamlit application development  
- Python backend engineering  
- Scikit-Learn forecasting  
- JSON schema validation  
- Asynchronous orchestration  

---

ğŸ“„ License
MIT License
